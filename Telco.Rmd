---
title: "Telco"
author: "Anuar Baisynov, Martin Bech, Bawer Betasi, Thomas Matzen, Marius McIntosh"
date: "2022-10-30"
output:
  html_document:
    toc: yes
    theme: spacelab
    highlight: tango
    toc_depth: 2
    toc_float: true
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Introduction

> Let's load the dataset first...

```{r}
telco <- read.csv("Telco-Customer-Churn.csv")
```

## Dataset Overview

> This dataset revolves itself around the **customer churn**. Inferably, the goal is thus to *use our progress in this course to predict the behavior of customers and thereby retain customers*, and perhaps provide preliminary findings for customer retention programs. As given away by the title, the data regards *"Telco"*, a hypothetical telecommunications company. Each row of the dataset is represented by a *customer*, and each column describes a *characteristic* of that customer and finally whether they have churned - or not. The dimensions of our dataset are: **`r dim(telco)`**, with **`r nrow(telco)`** rows and **`r ncol(telco)`** columns. Some descriptive statistics follow. 

```{r}
# Loading some data visualization packages...
suppressPackageStartupMessages(library(ggplot2))
suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(library(gmodels))
suppressPackageStartupMessages(library(caret))
suppressPackageStartupMessages(library(C50))
suppressPackageStartupMessages(library(class))
suppressPackageStartupMessages(library(kernlab))
```

```{r}
# Rather than describing each column, have a look at the first few columns to 
# understand the structure of the dataset. 
head(telco)

# Of course, we could run an str() here, but we haven't cleaned the dataset yet. This might lead to some 
# confusion, but just beware that the "chr" variables will be turned into factors later on. Similarly, 
# summary() is mostly pointless due to the "chr" columns. 
str(telco)

# Let's look at the proportions of churn in our total set.
ggplot(data = telco, aes(x = Churn, fill = Churn)) + 
  geom_bar() + 
  theme_linedraw() + 
  theme(legend.position = "none") +
  coord_flip()

# We already see that the "yes" option occurs rather frequently,
# at more or less 1750 instances, whereas no occurs some 5100 times. However, we don't need to 
# guesstimate here! Let's check the rows of yes and no for more specific data instead.
nrow(filter(telco, Churn == "Yes"))

nrow(filter(telco, Churn == "No"))

#Proportion of churn
nrow(filter(telco, Churn == "Yes"))/nrow(telco)

# To be precise, we have 1869 customers churned out and 5174 retained. 

# Next, we can already, as a de facto habit, look at whether the response variable churn differs per
# gender. 
ggplot(data = telco, aes(x = Churn, fill = Churn)) + 
  geom_bar() + 
  theme_linedraw() + 
  facet_wrap(~gender)
# Without doing any other analysis, it appears to be the case that gender 
# does not significantly affect churn. The two charts are near identical both in proportion and 
# in size, indicating that there might be a near 50-50 split of gender across the churn count.
```

For the next descriptive section, let's have a look at churn per some variables that - intuitively - could impact customer churn in a telecom firm.

```{r}
# Let's start off with the DeviceProtection and how that impacts Churn. 
# We can also look at the proportions of Gender here.
ggplot(data = telco, aes(x = Churn, fill = gender)) + 
  geom_bar() + 
  theme_linedraw() + 
  facet_wrap(~DeviceProtection)
# Interesting finding here: for the "No" option in DeviceProtection, the churn is relatively high 
# compared to the other values in this category. Again, the male-female split appears to be almost 
# exactly 50% in every kind of DeviceProtection.

# Next, how does the tenure affect churn? Perhaps customers who have stuck around for a while will 
# continue to stick around and thus need less discounts, etc.?
ggplot(telco, aes(Churn, tenure, fill = Churn)) + 
  geom_boxplot(alpha = 0.8) +
  theme_linedraw() + 
  theme(legend.position = "none")

# Interesting! The tenure appears to be higher for those who don't churn than those who will. 
# We'll later explore whether this effect is significant. Perhaps, loyalty up to the present 
# day is an indicator of future loyalty, after all.


# Up next, let's look at Churn by the Contract type. Perhaps a flexible month-to-month contract 
# will result in more inclination to churn...
ggplot(telco, aes(Churn, fill = Churn)) + 
  geom_bar() + 
  theme_linedraw() + 
  theme(legend.position = "none") +
  facet_wrap(~Contract)

# Our suspicion appears to be confirmed! The month-to-month contract has an extremely high churn rate
# relative to the other contract types... we'll later see whether this is significant through our
# regression model.

# And what about Churn per Payment Method used? Is flexibility here also a cause of more churn? 
ggplot(telco, aes(Churn, fill = Churn)) + 
  geom_bar() + 
  theme_linedraw() + 
  theme(legend.position = "none") +
  facet_wrap(~PaymentMethod)

# By simply looking at this, our eyes are set on the electronic check method. Although it has a 
# similar "no" count as the other payment systems, the "yes" count is much, much higher! This is
# something we'd have to consider in our regression.

# What about Paperless Billing? Would the lack of a physical paper (that presumably needs to be signed)
# cause more churn? 
ggplot(telco, aes(Churn, fill = Churn)) + 
  geom_bar() + 
  theme_linedraw() + 
  theme(legend.position = "none") +
  facet_wrap(~PaperlessBilling)

# Again, something interesting. We see that "yes" (so paperless) results in more than double in churn.
# Whether the effect is significant is unknown, though - again, something to consider later on.

# Finally, what about online security? Are people less inclined to cancel their plan if they are 
# secured from malicious intents by the telecom's security system? 

ggplot(telco, aes(x = Churn, fill = Churn)) + 
  geom_bar() + 
  theme_linedraw() + 
  theme(legend.position = "none") +
  facet_wrap(~OnlineSecurity)

# The differences are night and day here: it's clear that when people do not have security in their
# plan, their churn rate is much, much higher. Again, significance must be given a final validation
# in the regression model.
```

As may be noticeable, we have thus far merely considered the effect of a given predictor on the response variable, Churn. That said, we can - of course - also look at the interaction between different predictors through visualization. For now, we've only done this once to save time and space (see below).

```{r}
# Let's check the interaction between Online Security and the Contract Type! Perhaps people with a
# flexible contract also opt out of fancy features such as security? This could help us know what to
# cater to with our offering when providing a month-to-month contract...

ggplot(telco, aes(x = OnlineSecurity, fill = OnlineSecurity)) + 
  geom_bar() + 
  facet_wrap(~Contract) + 
  theme_linedraw() + 
  theme(axis.text.x = element_blank(),
        axis.title.x = element_blank(),
        axis.ticks.x = element_blank())

# Again, very interesting! This chart shows us a lot of things. First, it shows us that most people
# have a month-to-month contract. Beyond that, however, it tells us that most people in a monthly
# contract opt out of security! For the one-year contract, the choice for online security is 
# almost split equally (omitting the "no internet service" category), whereas most people opt 
# in for online security given a two-year contract.
```

Of course, we can do **a lot more** with data visualization. For now, however, we'll keep it as is. A brief conclusion will follow before we move on to the next section.

### Conclusion

Overall, we've seen a lot of interesting findings that indicates the dataset's and analytical opportunities to drive down the customer churn rate. Exposing these correlations by purely considering business intuition also indicates the importance of *not merely running the code, but also using business judgment to apply the analytical know-how*. 

> We've seen that **gender** appears to be (somewhat) **indifferent** to the churn rate, whereas the **contract type, device protection, online security, and payment method** are variables to look out for. Additionally, we've realized that **interaction effects must be considered**, too, as a **month-to-month contract is almost always paired with no online security**. *Regardless, it is almost certain that we've missed numerous important effects, and that's why we'll run numerous types of ML algorithms and regression models.*

## Business Questions - REWORD: REDUCE COSTS BY MINIMIZING CHURN
> In this project, we are trying to answer **one overarching question**:

> *Can we, at least with a 90% accuracy, predict whether a certain customer will churn based on their demographic characteristics and product preferences?* To this end, we plan to create machine learning algorithms that will determine whether a given customer will eventually stop buying our services or not. Applying our models to multiple test cases will help us derive an accuracy score.

<<<<<<< HEAD

> Overall, our goal is to develop targeted retention strategies for customers who are likely to churn.

## Preliminary Cleaning Effort
> In the upcoming section, the data is to be cleaned in order to prepare for data analysis, First, we start by deleting the "customerID" column as this column is irrelevant for our purpose (we're not trying to identify specific customers in our dataset). Following this, we factorize the columns with categorical data.

> After this, we go on to deal with the "TotalCharges" column. This column has some NA values, which we choose to replace by the median value of the column. We use the median value as this is outlier resistent and won't be influenced by the high/low values in the dataset. Lastly, we also update the Y-value to a numerical basis (Answer "No" will be a 0) to allow for better data analysis.

### Loading and Cleaning Data
```{r}
telco$customerID <- NULL

# Factorizing binary variables
telco$Churn <- as.factor(telco$Churn)
telco$PaperlessBilling <- as.factor(telco$PaperlessBilling)
telco$PhoneService <- as.factor(telco$PhoneService)
telco$Dependents <- as.factor(telco$Dependents)
telco$gender <- as.factor(telco$gender)
telco$SeniorCitizen <- as.factor(telco$SeniorCitizen)
telco$Partner <- as.factor(telco$Partner)

# Factorizing non-binary categorical variables
telco$PaymentMethod <- as.factor(telco$PaymentMethod)
telco$Contract <- as.factor(telco$Contract)
telco$InternetService <- as.factor(telco$InternetService)
telco$OnlineSecurity <- as.factor(telco$OnlineSecurity)
telco$OnlineBackup <- as.factor(telco$OnlineBackup)
telco$DeviceProtection <- as.factor(telco$DeviceProtection)
telco$TechSupport <- as.factor(telco$TechSupport)
telco$StreamingTV <- as.factor(telco$StreamingTV)
telco$StreamingMovies <- as.factor(telco$StreamingMovies)
telco$MultipleLines <- as.factor(telco$MultipleLines)

# Handling NA's
telco$TotalCharges[is.na(telco$TotalCharges)]<-median(telco$TotalCharges, na.rm=TRUE)

# Updating Y values
telco$Churn <- ifelse(telco$Churn == "No", 0, 1)

summary(telco)
str(telco)
```

### Normalization

```{r}
set.seed(12345)

# Convert all categorical variables into dummies
telco_modelm <- as.data.frame(model.matrix(~., -1, data = telco))
telco_modelm$`(Intercept)` <- NULL

# Normalize the data
normalize <- function(x) {
  return((x-min(x))/(max(x)-min(x)))
}
telco_normal <- as.data.frame(lapply(telco_modelm, normalize))

# Factorize the response variable
telco_normal$Churn <- as.factor(telco_normal$Churn)

# Create a label for our test set
telco_labels <- telco_normal$Churn
```

# First-Level Models
## Logistic Regression
### Simple LR
```{r}
set.seed(300)
library(caret)

# Customize the tuning process using trainControl() to alter resampling strategy
ctrl <- trainControl(method = "cv", number = 4,
                     selectionFunction = "oneSE")

# Build LR model with the set resampling strategy and grid parameters
lr1 <- train(Churn ~ ., data = telco_normal, method = "glm", metric = "Kappa", trControl = ctrl)

# Show summary of the model
summary(lr1)

# Predict the test set
lr1pred <- predict(lr1, telco_normal[-31])

lm1_cm <- confusionMatrix(data = as.factor(lr1pred), reference = as.factor(telco_labels), positive = "1")

# Show the confusion matrix
lm1_cm

# Get accuracy and kappa values
lm1_acc <- as.numeric(lm1_cm$overall['Accuracy'])
lm1_kappa <- as.numeric(lm1_cm$overall['Kappa'])
lm1_sens <- as.numeric(lm1_cm$byClass['Sensitivity'])
```
<br>
> The initial logistic regression gave us an accuracy score of``r lm1_acc`, a Kappa value of `r lm1_kappa`, and a sesntivity value of `r lm1_sens`.
> What immediately stands out in this model is the high number of **false negatives** where our model predicts that a customer would be retained (churn = 0), when it isn't the case in reality (i.e., they churned; churn = 1). Since this is an initial and relatively simple model, that is okay for now, as we will continue to work with this dataset to improve the prediction accuracy and bring down the false negatives simultaneously. This is the end of our code for the first intermediate submission.

### Improved LR
```{r}
set.seed(300)

# Build LR model with the set resampling strategy and grid parameters
lr2 <- train(Churn ~ SeniorCitizen1 + tenure + MultipleLinesYes + InternetServiceFiber.optic + InternetServiceNo + ContractOne.year + ContractTwo.year + PaperlessBillingYes + PaymentMethodElectronic.check + TotalCharges, data = telco_normal, method = "glm", metric = "Kappa", trControl = ctrl)

# Show summary of the model
summary(lr2)

# Predict the test set
lr2pred <- predict(lr2, telco_normal[-31])

lm2_cm <- confusionMatrix(data = as.factor(lr2pred), reference = as.factor(telco_labels), positive = "1")

# Show the confusion matrix
lm2_cm

# Get accuracy and kappa values
lm2_acc <- as.numeric(lm2_cm$overall['Accuracy'])
lm2_kappa <- as.numeric(lm2_cm$overall['Kappa'])
lm2_sens <- as.numeric(lm2_cm$byClass['Sensitivity'])
```
<br>
> The improved logistic regression gave us an accuracy score of``r lm2_acc`, a Kappa value of `r lm2_kappa`, and a sesntivity value of `r lm2_sens`.

## ANN
```{r, cache = TRUE}
set.seed(300)

# Set up a grid to test different hidden layers
grid <- expand.grid(size = c(1, 3), decay = c(0, 0.05))

# Build ANN model with the set resampling strategy and grid parameters
ann1 <- caret::train(Churn ~ ., data = telco_normal, method = "nnet", metric = "Kappa", trControl = ctrl, tuneGrid = grid)

# Show summary of the model
summary(ann1)

# Predict the test set
ann1pred <- predict(ann1, telco_normal[-31])

ann1_cm <- confusionMatrix(data = as.factor(ann1pred), reference = as.factor(telco_labels), positive = "1")

# Show the confusion matrix
ann1_cm

# Get accuracy and kappa values
ann1_acc <- as.numeric(ann1_cm$overall['Accuracy'])
ann1_kappa <- as.numeric(ann1_cm$overall['Kappa'])
ann1_sens <- as.numeric(ann1_cm$byClass['Sensitivity'])
```
<br>
> First ANN gave us an accuracy score of``r ann1_acc`, a Kappa value of `r ann1_kappa`, and a sesntivity value of `r ann1_sens`.

## kNN
```{r, cache = TRUE}
set.seed(300)

# Use expand.grid() to create grid of tuning parameters
grid <- expand.grid(k = seq(5, 100, 5))

# Build kNN models with the set resampling strategy and grid parameters
knn1 <- train(Churn ~ ., data = telco_normal, method = "knn",
           metric = "Kappa",
           trControl = ctrl,
           tuneGrid = grid)

# Predict the test set
knn1pred <- predict(knn1, telco_normal[-31])

# Create a confusion matrix
knn1_cm <- confusionMatrix(data = as.factor(knn1pred), reference = as.factor(telco_labels), positive = "1")

# Show the confusion matrix
knn1_cm

# Get accuracy and kappa values
knn1_acc <- as.numeric(knn1_cm$overall['Accuracy'])
knn1_kappa <- as.numeric(knn1_cm$overall['Kappa'])
knn1_sens <- as.numeric(knn1_cm$byClass['Sensitivity'])
```
<br>
> The kNN model gave us an accuracy score of``r knn1_acc`, a Kappa value of `r knn1_kappa`, and a sesntivity value of `r knn1_sens`.

## Decision Tree
### Simple DT
```{r, cache = TRUE}
set.seed(300)

## Create a simple tuned model with automated parameter tuning of C5.0 decision tree 
dt1 <- train(Churn ~ ., data = telco_normal, method = "C5.0")

# Apply the best candidate model to make predictions
dt1_pred <- predict(dt1, telco_normal[-31])

# Create a confusion matrix
dt1_cm <- confusionMatrix(data = as.factor(dt1_pred), reference = as.factor(telco_labels), positive = "1")

# Show the confusion matrix
dt1_cm

# Get accuracy and kappa values
dt1_acc <- as.numeric(dt1_cm$overall['Accuracy'])
dt1_kappa <- as.numeric(dt1_cm$overall['Kappa'])
dt1_sens <- as.numeric(dt1_cm$byClass['Sensitivity'])
```
<br>
> Our in initial Decision Tree model gave us an accuracy score of``r dt1_acc`, a Kappa value of `r dt1_kappa`, and a sesntivity value of `r dt1_sens`. We will now try to tune the hyperparameters of the model.

```{r, cache = TRUE}
set.seed(300)

# Use expand.grid() to create grid of tuning parameters
grid <- expand.grid(.model = "tree",
                    .trials = c(1, 5, 10, 15, 20, 25, 30),
                    .winnow = "FALSE")

# Customize train() with the control list and grid of parameters 
dt2 <- train(Churn ~ ., data = telco_normal, method = "C5.0",
           metric = "Kappa",
           trControl = ctrl,
           tuneGrid = grid)

# Apply the best candidate model to make predictions
dt2_pred <- predict(dt2, telco_normal[-31])

# Create a confusion matrix
dt2_cm <- confusionMatrix(data = as.factor(dt2_pred), reference = as.factor(telco_labels), positive = "1")

dt2_cm

# Get accuracy and kappa values
dt2_acc <- as.numeric(dt2_cm$overall['Accuracy'])
dt2_kappa <- as.numeric(dt2_cm$overall['Kappa'])
dt2_sens <- as.numeric(dt2_cm$byClass['Sensitivity'])
```
<br>
> The improved model model  saw an accuracy of `r dt2_acc`, a Kappa value of `r dt2_kappa` and a sensitivity value of `r dt2_sens`.

## Support Vector Machine (SVM) 
### SVM Linear
```{r, cache = TRUE}
set.seed(300)

# Customize train() with the control list and grid of parameters 
svm1 <- train(Churn ~ ., data = telco_normal, method = "svmLinear",
           metric = "Kappa",
           trControl = ctrl)

# Apply the best candidate model to make predictions
svm1_pred <- predict(svm1, telco_normal[-31])

# Create a confusion matrix
svm1_cm <- confusionMatrix(data = as.factor(svm1_pred), reference = as.factor(telco_labels), positive = "1")

# Show the confusion matrix
svm1_cm

# Get accuracy and kappa values
svm1_acc <- as.numeric(svm1_cm$overall['Accuracy'])
svm1_kappa <- as.numeric(svm1_cm$overall['Kappa'])
svm1_sens <- as.numeric(svm1_cm$byClass['Sensitivity'])
```
<br>
> The SVM model with a linear kernel achieved an accuracy of `r svm1_acc`, a Kappa value of `r svm1_kappa` and a sensitivity value of `r svm1_sens`.

### SVM Radial
```{r, cache = TRUE}
set.seed(300)

# Customize train() with the control list and grid of parameters 
svm2 <- train(Churn ~ ., data = telco_normal, method = "svmRadial",
           metric = "Kappa",
           trControl = ctrl)

# Apply the best candidate model to make predictions
svm2_pred <- predict(svm2, telco_normal[-31])

# Create a confusion matrix
svm2_cm <- confusionMatrix(data = as.factor(svm2_pred), reference = as.factor(telco_labels), positive = "1")

# Show the confusion matrix
svm2_cm

# Get accuracy and kappa values
svm2_acc <- as.numeric(svm2_cm$overall['Accuracy'])
svm2_kappa <- as.numeric(svm2_cm$overall['Kappa'])
svm2_sens <- as.numeric(svm2_cm$byClass['Sensitivity'])
```
<br>
> The SVM model with a radial kernel achieved an accuracy of `r svm2_acc`, a Kappa value of `r svm2_kappa` and a sensitivity value of `r svm2_sens`.

### SVM Poly
```{r, cache = TRUE}
set.seed(300)

# Customize train() with the control list and grid of parameters 
svm3 <- train(Churn ~ ., data = telco_normal, method = "svmPoly",
           metric = "Kappa",
           trControl = ctrl)

# Apply the best candidate model to make predictions
svm3_pred <- predict(svm3, telco_normal[-31])

# Create a confusion matrix
svm3_cm <- confusionMatrix(data = as.factor(svm3_pred), reference = as.factor(telco_labels), positive = "1")

# Show the confusion matrix
svm3_cm

# Get accuracy and kappa values
svm3_acc <- as.numeric(svm3_cm$overall['Accuracy'])
svm3_kappa <- as.numeric(svm3_cm$overall['Kappa'])
svm3_sens <- as.numeric(svm3_cm$byClass['Sensitivity'])
```
<br>
> The SVM model with a poly kernel achieved an accuracy of `r svm3_acc`, a Kappa value of `r svm3_kappa` and a sensitivity value of `r svm3_sens`.

## First-Layer Results 

```{r}
# Combine all accuracy, kappa, and sensitivity values
accuracy <- c(lm1_acc, lm2_acc, ann1_acc, knn1_acc, dt1_acc, dt2_acc, svm1_acc, svm2_acc, svm3_acc)
kappa <- c(lm1_kappa, lm2_kappa, ann1_kappa, knn1_kappa, dt1_kappa, dt2_kappa, svm1_kappa, svm2_kappa, svm3_kappa)
sens <- c(lm1_sens, lm2_sens, ann1_sens, knn1_sens, dt1_sens, dt2_sens, svm1_sens, svm2_sens, svm3_sens)

# Create a dataframe with all values
results <- data.frame(accuracy, kappa, sens)
rownames(results) <- c("LR1", "LR2", "ANN1", "kNN1", "DT1", "DT2", "SVM1", "SVM2", "SVM3")
colnames(results) <- c("Accuracy", "Kappa", "Sensitivity")

# Show the created dataframe
results
```
<br>
> With our first-layer results we decide which models we choose for the second-level model. We make our decision based on the Kappa score for the respective models. Using logistic regression the first model has bott the highest accuracy and Kappa, `r lm1_acc` and `r lm1_kappa`. Regarding our ANN and kNN models there are only one of each and both will be used in our second-level model. It is worth noting that these models have the highest sensitivity of all models with ANN having a value of `r ann1_sens` and kNN having a value of `r knn1_acc`. Our two decision tree models share a similiar accuracy, however the second model has both a higher Kappa, `r dt2_kappa`, and sensitivity, `r dt2_sens`, which is why we are continuing further with this model. For the support vector machine models, SVM, we have utilised three models while using different kernels for each model. The model using radial kernel achieved the highest kappa value of `r svm2_kappa`, and is therefore the one we are using in the second-level model

# Second-Level Model

```{r}
# Combine everything into one big dataframe for our stacked model.
combineddf <- data.frame(lr1pred, ann1pred, knn1pred, dt2_pred, svm2_pred, telco_labels)

# Change colnames to something more intuitive
colnames(combineddf) <- c("LR", "ANN", "kNN", "DT", "SVM", "Actual")

# Show summary statistics for the dataframe
summary(combineddf)
```

## Simple Stacked Model
```{r, cache = TRUE}
set.seed(300)

# Use expand.grid() to create grid of tuning parameters
grid <- expand.grid(.model = "tree",
                    .trials = c(1, 5, 10, 15, 20, 25, 30),
                    .winnow = "FALSE")

# Customize train() with the control list and grid of parameters 
dt_comb <- train(Actual ~ ., data = combineddf, method = "C5.0",
           metric = "Kappa",
           trControl = ctrl,
           tuneGrid = grid)

# Apply the best candidate model to make predictions
dt_comb_pred <- predict(dt_comb, combineddf[-6])

# Create a confusion matrix
dt_comb_cm <- confusionMatrix(data = as.factor(dt_comb_pred), reference = as.factor(combineddf$Actual), positive = "1")

# Show the confusion matrix
dt_comb_cm

# Get accuracy and kappa values
dt_comb_acc <- as.numeric(dt_comb_cm$overall['Accuracy'])
dt_comb_kappa <- as.numeric(dt_comb_cm$overall['Kappa'])
dt_comb_sens <- as.numeric(dt_comb_cm$byClass['Sensitivity'])
```
<br>
> The combined model achieved an accuracy of `r dt_comb_acc`, a Kappa value of `r dt_comb_kappa`, and a sensitivity score of `r dt_comb_sens`. 

## Results Comparison - NEED TO UPDATE AND ELABORATE
```{r}
# Combine all accuracy, kappa, and sensitivity values
accuracy <- c(lm1_acc, ann1_acc, knn1_acc, dt2_acc, svm2_acc, dt_comb_acc)
kappa <- c(lm1_kappa, ann1_kappa, knn1_kappa, dt2_kappa, svm2_kappa, dt_comb_kappa)
sens <- c(lm1_sens, ann1_sens, knn1_sens, dt2_sens, svm2_sens, dt_comb_sens)

# Create a dataframe with all values
results <- data.frame(accuracy, kappa, sens)
rownames(results) <- c("LR1", "ANN1", "kNN1", "DT2", "SVM2", "Combined")
colnames(results) <- c("Accuracy", "Kappa", "Sensitivity")

# Show the created dataframe
results
```
<br>
> Thus, the combined model has achieved the highest accuracy score and Kappa value, but its sensitivity is lower than that of ANN and kNN. In this project, we are specifically interested in minimizing the number of false negatives - unnoticed instances of churn. We will try to reduce that number (and thus increase the combined model's sensitivity score) using an **error cost matrix**.

## Error Cost Matrix
```{r}
library(C50)

# Do a 70-30 train-test split
combined_train <- combineddf[1:(nrow(combineddf)*0.7),]
combined_test <- combineddf[((nrow(combineddf)*0.7)+1):nrow(combineddf),]

# Create a cost matrix
error_cost <- matrix(c(0, 1, 4, 0), nrow = 2)

# Apply the cost matrix to the tree
dt2_comb <- C5.0(as.factor(Actual) ~ ., data = combined_train, costs = error_cost)
dt2_comb_pred <- predict(dt2_comb, combined_test)

# Build a confusion matrix
dt2_comb_cm <- confusionMatrix(data = as.factor(dt2_comb_pred), reference = as.factor(combined_test$Actual), positive = "1")

# Show the confusion matrix
dt2_comb_cm

# Get accuracy and kappa values
dt2_comb_acc <- as.numeric(dt2_comb_cm$overall['Accuracy'])
dt2_comb_kappa <- as.numeric(dt2_comb_cm$overall['Kappa'])
dt2_comb_sens <- as.numeric(dt2_comb_cm$byClass['Sensitivity'])
```

<br>
> By applying a cost matrix, we can increase our senstivity score to `r dt2_comb_sens` but the accuracy rate falls to `r dt2_comb_acc`. That is primarily because we are increasing the number of false positives. The goal of our project is to identify individuals with a high risk of churn and target them with retention strategies. Next, we will look into the costs associated with losing customers as well as running such retention strategies. With that information, we will be able to estimate how much money the company will be able to save by targeting only customers who are at risk of churn according to the model.

# Business Implications
## Model Results
```{r}
# Extract numbers from the table
TN <- as.numeric(dt2_comb_cm$table)[1]
FP <- as.numeric(dt2_comb_cm$table)[2]
FN <- as.numeric(dt2_comb_cm$table)[3]
TP <- as.numeric(dt2_comb_cm$table)[4]

# Find the total number of customers in the sample
all_customers <- TP + FN + FP + TN

# Find the number and percentage of customers who are indeed going to churn
customers_at_risk <- TP + FN
customers_at_risk_perc <- customers_at_risk / all_customers

# Find the number and percentage of customers whom our model recommends targeting
customers_targeted <- TP + FP
customers_targeted_perc <- customers_targeted / all_customers

# Find the number of churning customers not identified by the model
missed_churn <- FN

# Find the percentage of identified customers at risk
identified_churn_perc <- TP / customers_at_risk
```
<br>
> In our sample of `r all_customers` customers, `r customers_at_risk` people were actually going to churn (`r round(customers_at_risk_perc*100,2)`% of the sample). Our model recommends offering retention programs to `r customers_targeted` people (`r round(customers_targeted_perc*100,2)`%). This number includes `r TP` customers who are indeed at risk of churn and `r FP` people who were not going to churn. Although the recommended number of unnecessary retention offerings seems relatively high at first glance, our model allows us to catch and, hopefully, prevent `r round(identified_churn_perc*100,2)`% of all churns. We argue that the expenses associated with running this many retention programs is justified by the prevention of profit losses due to customer churn. In the following sections, we will extrapolate these results to the entire population from the dataset to calculate the impact of the model.     

## Assumptions
### Financial Assumptions
```{r}
# Calculate the average monthly ARPU as the mean of all monthly charges in the dataset
telco_monthly_arpu <- mean(telco$MonthlyCharges)

# Make a research-informed assumption about TelCo's gross margin
telco_gross_profit_margin <- 0.75

# Calculate the average monthly gross profit per user
telco_monthly_gppu <- telco_monthly_arpu * telco_gross_profit_margin

# Make an assumption about the cost of a retention program per user
retention_cost_per_user <- 50
```
<br>
The average TTM net profit margin for companies in the telecommunications sector, as of Q3 2022, is approximately 12.3%. Gross profit margins for the sector can run as high as 70% to 80%, but extremely high overhead expenses erode much of that initial profit balance ("Communications Services Industry Profitability by Quarter, Gross, Operating and Net Margin From Q3 2022"). Assuming a 75% gross profit margin, TelCo's monthly gross profit per user makes up $`r round(telco_monthly_gppu,2)`. 

### Lifetime assumptions
```{r}
# Calculate the average tenure of churned customers
churn_tenure = mean(filter(telco, Churn == 1)$tenure)

# Use the average tenure of loyal customers as a proxy for the total customer lifetime
non_churn_tenure = mean(filter(telco, Churn == 0)$tenure)

# Calculate the lost lifespan of churned customers
lost_lifespan = non_churn_tenure - churn_tenure
```
<br>

## Profitability Implications
Next, we are going to consider TelCo's profitability in three scenarios: **(1) no retention**, **(2) random retention**, and **(3) model-informed retention**.

### Scenario 1 - Base Scenario - No Retention
**The first scenario implies not offering any retention programs and allowing organic churn.**
```{r}
# Calculate the lost profit per churned customer
lost_profit_per_churn <- lost_lifespan * telco_monthly_gppu

# Calculate the total lost profit with no retention strategies
lost_profit_no_retention <- lost_profit_per_churn * nrow(filter(telco, Churn == 1))
```
<br>
If we do not offer any retention programs, TelCo will lose $`r lost_profit_no_retention` in foregone profits due to churn.

### Scenario 2 - Random Retention
**The second scenario implies offering retention programs to a randomly selected sample of 30% of all customers.**
```{r}
# Calculate the expected percentage of identified churn
randomly_identified_churn_perc <- nrow(filter(telco, Churn == 1))/nrow(telco)

# Assume targeted sample size for random targeting
random_sample_size <- 0.3

# Find number of targeted users
randomly_targeted_users <- round(random_sample_size * nrow(telco))

# Calculate the number of identified customers at risk
random_identified_churn <- round(nrow(telco) * random_sample_size * randomly_identified_churn_perc)

# Create a sensitivity model to find profitability at various retention program success rates
retained_customers_vector <- c()
profits_vector <- c()
success_rates <- seq(0, 1, 0.1)
for (rate in success_rates) {
  retained_customers <- round(random_identified_churn * rate)
  retained_customers_vector <- append(retained_customers_vector, retained_customers)
  regained_profit <- round(retained_customers * lost_profit_per_churn)
  total_retention_costs <- retention_cost_per_user * randomly_targeted_users
  remaining_churn_losses <- (nrow(filter(telco, Churn == 1)) - retained_customers) * lost_profit_per_churn
  bottom_line <- regained_profit - total_retention_costs - remaining_churn_losses
  profits_vector <- append(profits_vector, bottom_line)
}

# Create and plot a dataframe with sensitivity results
random_sensitivity_df <- data.frame(success_rates, retained_customers_vector, profits_vector)
random_sensitivity_df$relative_profit <- random_sensitivity_df$profits_vector + lost_profit_no_retention
colnames(random_sensitivity_df) <- c("Success Rates", "Retained Customers", "Bottom Line", "P/L Relative to Base Scenario")
random_sensitivity_df
ggplot(random_sensitivity_df, aes(x = success_rates, y = profits_vector)) + geom_line()
```
<br>
In an extreme optimistic subscenario (100% success rate), randomly offering retention programs to 30% of customers would help TelCo retain `r random_sensitivity_df[11, 2]` customers and thus gain $`r random_sensitivity_df[11, 3]+lost_profit_no_retention` in profits relative to the base scenario. 
In an extreme pessimistic subscenario (0% success rate), randomly offering retention programs to 30% of customers would help TelCo retain `r random_sensitivity_df[1, 2]` customers while also leading to additional losses of $`r -random_sensitivity_df[1, 3]-lost_profit_no_retention` relative to the base scenario due retention expenses.

### Scenario 3 - Model-Informed Retention
**The third scenario implies offering retention programs to all customers who, according to our model, are likely to churn.**
```{r}
# Find number of targeted users
model_targeted_customers <- round(customers_targeted_perc * nrow(telco))

# Calculate the number of identified customers at risk
model_identified_churn <- round(nrow(filter(telco, Churn == 1)) * identified_churn_perc)

# Create a sensitivity model to find profitability at various retention program success rates
retained_customers_vector <- c()
profits_vector <- c()
success_rates <- seq(0, 1, 0.1)
for (rate in success_rates) {
  retained_customers <- round(model_identified_churn * rate)
  retained_customers_vector <- append(retained_customers_vector, retained_customers)
  
  regained_profit <- round(retained_customers * lost_profit_per_churn)
  total_retention_costs <- retention_cost_per_user * model_targeted_customers
  remaining_churn_losses <- (nrow(filter(telco, Churn == 1)) - retained_customers) * lost_profit_per_churn
  bottom_line <- regained_profit - total_retention_costs - remaining_churn_losses
  profits_vector <- append(profits_vector, bottom_line)
}

# Create and plot a dataframe with sensitivity results
model_sensitivity_df <- data.frame(success_rates, retained_customers_vector, profits_vector)
model_sensitivity_df$relative_profit <- model_sensitivity_df$profits_vector + lost_profit_no_retention
colnames(model_sensitivity_df) <- c("Success Rates", "Retained Customers", "Bottom Line", "P/L Relative to Base Scenario")
model_sensitivity_df
ggplot(model_sensitivity_df, aes(x = success_rates, y = profits_vector)) + geom_line()
```
<br>
In an extreme optimistic subscenario (100% success rate), following the model's recommendations would help TelCo retain `r model_sensitivity_df[11, 2]` customers and thus gain $`r model_sensitivity_df[11, 3]+lost_profit_no_retention` in profits relative to the base scenario. 
In an extreme pessimistic subscenario (0% success rate), following the model's recommendations would help TelCo retain `r model_sensitivity_df[1, 2]` customers while also leading to additional losses of $`r -model_sensitivity_df[1, 3]-lost_profit_no_retention` relative to the base scenario due retention expenses.

## Conclusion - NEED TO WRITE


# References
* “Communications Services Industry Profitability by Quarter, Gross, Operating and Net Margin From 3 Q 2022.” Communications Services Industry Profitability by Quarter, Gross, Operating and Net Margin From 3 Q 2022, csimarket.com/Industry/industry_Profitability_Ratios.php?ind=905. Accessed 5 Dec. 2022.

