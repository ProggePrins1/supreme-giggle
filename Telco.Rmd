---
title: "Telco"
author: "Anuar Baisynov, Martin Bech, Bawer Betasi, Thomas Matzen, Marius McIntosh"
date: "2022-10-30"
output:
  html_document:
    toc: yes
    theme: spacelab
    highlight: tango
    toc_depth: 2
    toc_float: true
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Introduction

> Let's load the dataset first...

```{r}
telco <- read.csv("Telco-Customer-Churn.csv")
```

## Dataset Overview

> This dataset revolves itself around the **customer churn**. Inferably, the goal is thus to *use our progress in this course to predict the behavior of customers and thereby retain customers*, and perhaps provide preliminary findings for customer retention programs. As given away by the title, the data regards *"Telco"*, a hypothetical telecommunications company. Each row of the dataset is represented by a *customer*, and each column describes a *characteristic* of that customer and finally whether they have churned - or not. The dimensions of our dataset are: **`r dim(telco)`**, with **`r nrow(telco)`** rows and **`r ncol(telco)`** columns. Some descriptive statistics follow. 

```{r}
# Loading some data visualization packages...
suppressPackageStartupMessages(library(ggplot2))
suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(library(gmodels))
suppressPackageStartupMessages(library(caret))
suppressPackageStartupMessages(library(C50))
suppressPackageStartupMessages(library(class))
suppressPackageStartupMessages(library(kernlab))
```

```{r}
# Rather than describing each column, have a look at the first few columns to 
# understand the structure of the dataset. 
head(telco)

# Of course, we could run an str() here, but we haven't cleaned the dataset yet. This might lead to some 
# confusion, but just beware that the "chr" variables will be turned into factors later on. Similarly, 
# summary() is mostly pointless due to the "chr" columns. 
str(telco)

# Let's look at the proportions of churn in our total set.
ggplot(data = telco, aes(x = Churn, fill = Churn)) + 
  geom_bar() + 
  theme_linedraw() + 
  theme(legend.position = "none") +
  coord_flip()

# We already see that the "yes" option occurs rather frequently,
# at more or less 1750 instances, whereas no occurs some 5100 times. However, we don't need to 
# guesstimate here! Let's check the rows of yes and no for more specific data instead.
nrow(filter(telco, Churn == "Yes"))

nrow(filter(telco, Churn == "No"))

#Proportion of churn
nrow(filter(telco, Churn == "Yes"))/nrow(telco)

# To be precise, we have 1869 customers churned out and 5174 retained. 

# Next, we can already, as a de facto habit, look at whether the response variable churn differs per
# gender. 
ggplot(data = telco, aes(x = Churn, fill = Churn)) + 
  geom_bar() + 
  theme_linedraw() + 
  facet_wrap(~gender)
# Without doing any other analysis, it appears to be the case that gender 
# does not significantly affect churn. The two charts are near identical both in proportion and 
# in size, indicating that there might be a near 50-50 split of gender across the churn count.
```

For the next descriptive section, let's have a look at churn per some variables that - intuitively - could impact customer churn in a telecom firm.

```{r}
# Let's start off with the DeviceProtection and how that impacts Churn. 
# We can also look at the proportions of Gender here.
ggplot(data = telco, aes(x = Churn, fill = gender)) + 
  geom_bar() + 
  theme_linedraw() + 
  facet_wrap(~DeviceProtection)
# Interesting finding here: for the "No" option in DeviceProtection, the churn is relatively high 
# compared to the other values in this category. Again, the male-female split appears to be almost 
# exactly 50% in every kind of DeviceProtection.

# Next, how does the tenure affect churn? Perhaps customers who have stuck around for a while will 
# continue to stick around and thus need less discounts, etc.?
ggplot(telco, aes(Churn, tenure, fill = Churn)) + 
  geom_boxplot(alpha = 0.8) +
  theme_linedraw() + 
  theme(legend.position = "none")

# Interesting! The tenure appears to be higher for those who don't churn than those who will. 
# We'll later explore whether this effect is significant. Perhaps, loyalty up to the present 
# day is an indicator of future loyalty, after all.


# Up next, let's look at Churn by the Contract type. Perhaps a flexible month-to-month contract 
# will result in more inclination to churn...
ggplot(telco, aes(Churn, fill = Churn)) + 
  geom_bar() + 
  theme_linedraw() + 
  theme(legend.position = "none") +
  facet_wrap(~Contract)

# Our suspicion appears to be confirmed! The month-to-month contract has an extremely high churn rate
# relative to the other contract types... we'll later see whether this is significant through our
# regression model.

# And what about Churn per Payment Method used? Is flexibility here also a cause of more churn? 
ggplot(telco, aes(Churn, fill = Churn)) + 
  geom_bar() + 
  theme_linedraw() + 
  theme(legend.position = "none") +
  facet_wrap(~PaymentMethod)

# By simply looking at this, our eyes are set on the electronic check method. Although it has a 
# similar "no" count as the other payment systems, the "yes" count is much, much higher! This is
# something we'd have to consider in our regression.

# What about Paperless Billing? Would the lack of a physical paper (that presumably needs to be signed)
# cause more churn? 
ggplot(telco, aes(Churn, fill = Churn)) + 
  geom_bar() + 
  theme_linedraw() + 
  theme(legend.position = "none") +
  facet_wrap(~PaperlessBilling)

# Again, something interesting. We see that "yes" (so paperless) results in more than double in churn.
# Whether the effect is significant is unknown, though - again, something to consider later on.

# Finally, what about online security? Are people less inclined to cancel their plan if they are 
# secured from malicious intents by the telecom's security system? 

ggplot(telco, aes(x = Churn, fill = Churn)) + 
  geom_bar() + 
  theme_linedraw() + 
  theme(legend.position = "none") +
  facet_wrap(~OnlineSecurity)

# The differences are night and day here: it's clear that when people do not have security in their
# plan, their churn rate is much, much higher. Again, significance must be given a final validation
# in the regression model.
```

As may be noticeable, we have thus far merely considered the effect of a given predictor on the response variable, Churn. That said, we can - of course - also look at the interaction between different predictors through visualization. For now, we've only done this once to save time and space (see below).

```{r}
# Let's check the interaction between Online Security and the Contract Type! Perhaps people with a
# flexible contract also opt out of fancy features such as security? This could help us know what to
# cater to with our offering when providing a month-to-month contract...

ggplot(telco, aes(x = OnlineSecurity, fill = OnlineSecurity)) + 
  geom_bar() + 
  facet_wrap(~Contract) + 
  theme_linedraw() + 
  theme(axis.text.x = element_blank(),
        axis.title.x = element_blank(),
        axis.ticks.x = element_blank())

# Again, very interesting! This chart shows us a lot of things. First, it shows us that most people
# have a month-to-month contract. Beyond that, however, it tells us that most people in a monthly
# contract opt out of security! For the one-year contract, the choice for online security is 
# almost split equally (omitting the "no internet service" category), whereas most people opt 
# in for online security given a two-year contract.
```

Of course, we can do **a lot more** with data visualization. For now, however, we'll keep it as is. A brief conclusion will follow before we move on to the next section.

### Conclusion

Overall, we've seen a lot of interesting findings that indicates the dataset's and analytical opportunities to drive down the customer churn rate. Exposing these correlations by purely considering business intuition also indicates the importance of *not merely running the code, but also using business judgment to apply the analytical know-how*. 

> We've seen that **gender** appears to be (somewhat) **indifferent** to the churn rate, whereas the **contract type, device protection, online security, and payment method** are variables to look out for. Additionally, we've realized that **interaction effects must be considered**, too, as a **month-to-month contract is almost always paired with no online security**. *Regardless, it is almost certain that we've missed numerous important effects, and that's why we'll run numerous types of ML algorithms and regression models.*

## Business Questions
> In this project, we are trying to answer **two overarching questions**:

> 1) *Are there customer groups with certain demographic characteristics or product preferences that tend to stop buying our services more often than others?* We will try to investigate this in two ways. First, we will try to explore differences in churn rates across various explanatory features in our dataset (Exploratory Data Analysis section). Next, we will try to aggregate the data by running the k-means clustering algorithm to identify customer segments with the highest churn rate (Clustering section).

> 2) *Can we, at least with a 90% accuracy, predict whether a certain customer will churn based on their demographic characteristics and product preferences?* To this end, we plan to create machine learning algorithms that will determine whether a given customer will eventually stop buying our services or not. Applying our models to multiple test cases will help us derive an accuracy score.

<<<<<<< HEAD
> Overall, our goal is to improve retention among both current and future customers. We can identify segments that have an unusually high churn rates and try to develop targeted retention strategies for them. 
=======
> Overall, our goal is two-fold: identify current customer segments that have an unusually high churn rate and try to develop targeted retention strategies for them.
>>>>>>> 91483c6fe216f4245fbaa593ef6be04881c919fb

## Preliminary Cleaning Effort
> In the upcoming section, the data is to be cleaned in order to prepare for data analysis, First, we start by deleting the "customerID" column as this column is irrelevant for our purpose (we're not trying to identify specific customers in our dataset). Following this, we factorize the columns with categorical data.

> After this, we go on to deal with the "TotalCharges" column. This column has some NA values, which we choose to replace by the median value of the column. We use the median value as this is outlier resistent and won't be influenced by the high/low values in the dataset. Lastly, we also update the Y-value to a numerical basis (Answer "No" will be a 0) to allow for better data analysis.

### Loading and Cleaning Data
```{r}
telco$customerID <- NULL

# Factorizing binary variables
telco$Churn <- as.factor(telco$Churn)
telco$PaperlessBilling <- as.factor(telco$PaperlessBilling)
telco$PhoneService <- as.factor(telco$PhoneService)
telco$Dependents <- as.factor(telco$Dependents)
telco$gender <- as.factor(telco$gender)
telco$SeniorCitizen <- as.factor(telco$SeniorCitizen)
telco$Partner <- as.factor(telco$Partner)

# Factorizing non-binary categorical variables
telco$PaymentMethod <- as.factor(telco$PaymentMethod)
telco$Contract <- as.factor(telco$Contract)
telco$InternetService <- as.factor(telco$InternetService)
telco$OnlineSecurity <- as.factor(telco$OnlineSecurity)
telco$OnlineBackup <- as.factor(telco$OnlineBackup)
telco$DeviceProtection <- as.factor(telco$DeviceProtection)
telco$TechSupport <- as.factor(telco$TechSupport)
telco$StreamingTV <- as.factor(telco$StreamingTV)
telco$StreamingMovies <- as.factor(telco$StreamingMovies)
telco$MultipleLines <- as.factor(telco$MultipleLines)

# Handling NA's
telco$TotalCharges[is.na(telco$TotalCharges)]<-median(telco$TotalCharges, na.rm=TRUE)

# Updating Y values
telco$Churn <- ifelse(telco$Churn == "No", 0, 1)

summary(telco)
str(telco)
```


## <a id="train_test"></a>Creating Train and Test Sets

```{r}
set.seed(12345)
telco_modelm <- as.data.frame(model.matrix(~., -1, data = telco))
telco_modelm$`(Intercept)` <- NULL

normalize <- function(x) {
  return((x-min(x))/(max(x)-min(x)))
}

telco_normal <- as.data.frame(lapply(telco_modelm, normalize))

telco_sample <- telco[sample(nrow(telco)),]

telco_train  <- telco[1:(0.75*nrow(telco)), ]
telco_test   <- telco[(0.75*nrow(telco)+1):nrow(telco),]

telco_train_normal <- telco_normal[1:(0.75*nrow(telco)), ]
telco_test_normal <- telco_normal[(0.75*nrow(telco)+1):nrow(telco),]

# Creating a label for our test set
telco_test_labels <- telco_test$Churn

# Removing the response variable
telco_test$Churn <- NULL
```

## First Logistic Regression for EDA (First Intermediate Deliverable)

```{r}
predictionmodelt1 <- glm(Churn ~ ., data = telco_train, family = "binomial")
summary(predictionmodelt1)

# Let's now run a model with only the significant variables, with a p-value threshold of 0.05.

predictionmodelt2 <- glm(Churn ~ tenure + InternetService + Contract + PaperlessBilling + PaymentMethod + TotalCharges, data = telco_train, family = "binomial")

summary(predictionmodelt2)

# We can can now run our initial prediction based on this simple logistic regression. 
# We'll use a threshold of 50% here.

lm_pred <- predict(predictionmodelt2, newdata = telco_test, type = "response")
binary_lm <- ifelse(lm_pred < 0.5, 0, 1)

CrossTable(telco_test_labels, binary_lm, prop.chisq = FALSE)

lm_cm <- confusionMatrix(data = as.factor(binary_lm), reference = as.factor(telco_test_labels), positive = "1")

# Show the confusion matrix
lm_cm

# Get accuracy and kappa values
lm_acc <- as.numeric(lm_cm$overall['Accuracy'])
lm_kappa <- as.numeric(lm_cm$overall['Kappa'])
lm_sens <- as.numeric(lm_cm$byClass['Sensitivity'])
```
<br>
> What immediately stands out in this model is the high number of **false negatives (245)** where our model predicts that a customer would be retained (churn = 0), when it isn't the case in reality (i.e., they churned; churn = 1). Since this is an initial and relatively simple model, that is okay for now, as we will continue to work with this dataset to improve the prediction accuracy and bring down the false negatives simultaneously. This is the end of our code for the first intermediate submission.

# Answering Business Question 1

For this section, we will determine whether there are certain demographics or product preferences that result in people leaving our products less/more often. To answer this question, we will use k-means clustering and analyze the different characteristics of each segment.

> The question is to be answered later in the semester. For the second intermediate deliverable, we are only building a stacked model based on the entire dataset without breaking it down into clusters.

# Answering Business Question 2

For this section, we will check if we can predict with 90% accuracy whether someone will churn out of our telco offering. To do so, we will first build an ANN, kNN, LR, SVM, and DT prediction models, and then use those outputs to create an ensemble model. 

> The question is to be answered later in the semester. For the second intermediate deliverable, we are only building a stacked model based on the entire dataset without breaking it down into clusters.

# First-Level Models
## ANN
### Simple ANN
```{r, cache = TRUE}
library(neuralnet)

# Run ANN
telco_ann <- neuralnet(Churn ~., data = telco_train_normal, hidden = 1)

# Create a Plot
plot(telco_ann)

# Compute Predictions
ann_results <- compute(telco_ann, telco_test_normal[-31])

# Store net.result in variable and show overview
predicted_churn <- ann_results$net.result
summary(predicted_churn)

# Run a binary prediction
binary_ann <- as.numeric(ifelse(predicted_churn < 0.4, 0, 1))
# The binary threshold of 0.4 maximizes the Kappa value

# CrossTable and Confusion Matrix

CrossTable(telco_test_normal$Churn, binary_ann, prop.chisq = FALSE)

ann_cm <- confusionMatrix(data = as.factor(binary_ann), reference = as.factor(telco_test_normal$Churn), positive = "1")

ann_cm

# Get accuracy and kappa values
ann_acc <- as.numeric(ann_cm$overall['Accuracy'])
ann_kappa <- as.numeric(ann_cm$overall['Kappa'])
ann_sens <- as.numeric(ann_cm$byClass['Sensitivity'])
```
<br>
> First ANN gave us an accuracy score of``r ann_acc` and a Kappa value of `r ann_kappa`. This represents a slight improvement from what we saw with our simple logistic regression. We will now try to increase the number of hidden layers in the model.

### Improved ANN (5 Hidden Layers)
```{r, cache = TRUE}
# Run ANN
telco_ann2 <- neuralnet(Churn ~., data = telco_train_normal, hidden = 5, stepmax = 1e7)

# Create a Plot
plot(telco_ann2)

# Compute Predictions
ann_results2 <- compute(telco_ann2, telco_test_normal[-31])

# Store net.result in variable and show overview
ann_predicted_churn2 <- ann_results2$net.result
summary(ann_predicted_churn2)

# Run a binary prediction
binary_ann2 <- as.numeric(ifelse(ann_predicted_churn2 < 0.5, 0, 1))

# CrossTable and Confusion Matrix

CrossTable(telco_test_normal$Churn, binary_ann2, prop.chisq = FALSE)

ann2_cm <- confusionMatrix(data = as.factor(binary_ann2), reference = as.factor(telco_test_normal$Churn), positive = "1")

ann2_cm

# Get accuracy and kappa values
ann2_acc <- as.numeric(ann2_cm$overall['Accuracy'])
ann2_kappa <- as.numeric(ann2_cm$overall['Kappa'])
ann2_sens <- as.numeric(ann2_cm$byClass['Sensitivity'])
```
<br>
> A model with 5 hidden layers achieves an accuracy of `r ann2_acc` with a binary threshold of 0.5. The Kappa value achieved with that threshold was `r ann2_kappa`, so that is even lower than what we saw with a model that had only one hidden layer. 

### Improved ANN (3 Hidden Layers)
```{r, cache = TRUE}
# Run ANN
telco_ann3 <- neuralnet(Churn ~., data = telco_train_normal, hidden = 3, stepmax = 1e7)

# Create a Plot
plot(telco_ann3)

# Compute Predictions
ann_results3 <- compute(telco_ann3, telco_test_normal[-31])

# Store net.result in variable and show overview
ann_predicted_churn3 <- ann_results3$net.result
ann_predicted_churn3 <- as.numeric(ann_predicted_churn3)

# Run a binary prediction
binary_ann3 <- as.numeric(ifelse(ann_predicted_churn3 < 0.4, 0, 1))

# CrossTable and Confusion Matrix
CrossTable(telco_test_normal$Churn, binary_ann3, prop.chisq = FALSE)

ann3_cm <- confusionMatrix(data = as.factor(binary_ann3), reference = as.factor(telco_test_normal$Churn), positive = "1")

ann3_cm

# Get accuracy and kappa values
ann3_acc <- as.numeric(ann3_cm$overall['Accuracy'])
ann3_kappa <- as.numeric(ann3_cm$overall['Kappa'])
ann3_sens <- as.numeric(ann3_cm$byClass['Sensitivity'])
```
<br>
> With three hidden layers, we are seeing much better results. With a binary threshold of 0.4, the accuracy is `r ann3_acc` and the Kappa value is `r ann3_kappa`. We will be using this model moving forward.

## kNN
### Simple kNN model
```{r, cache = TRUE}
library(caret)

# Create a simple tuned model
set.seed(300)
telco_train_normal$Churn <- as.factor(telco_train_normal$Churn)
knn <- train(Churn ~ ., data = telco_train_normal, method = "knn")

# Apply the best kNN candidate model to make predictions
knn_pred <- predict(knn, telco_test_normal[-31])

# Create a confusion matrix
knn_cm <- confusionMatrix(data = knn_pred, reference = as.factor(telco_test_normal$Churn), positive = "1")

knn_cm

# Get accuracy and kappa values
knn_acc <- as.numeric(knn_cm$overall['Accuracy'])
knn_kappa <- as.numeric(knn_cm$overall['Kappa'])
knn_sens <- as.numeric(knn_cm$byClass['Sensitivity'])
```
<br>
> The simple model achieved an accuracy of `r knn_acc` and a Kappa value of `r knn_kappa`. We will try to improve these results by tuning hyperparameters, specifically the value of k.

### Improved kNN model
```{r, cache = TRUE}
set.seed(300)

# Customize the tuning process using trainControl() to alter resampling strategy
ctrl <- trainControl(method = "cv", number = 10,
                     selectionFunction = "oneSE")

# Use expand.grid() to create grid of tuning parameters
grid <- expand.grid(k = seq(5, 100, 5))

# Build kNN models with the set resampling strategy and grid parameters
knn2 <- train(Churn ~ ., data = telco_train_normal, method = "knn",
           metric = "Kappa",
           trControl = ctrl,
           tuneGrid = grid)

# Predict the test set
knn_pred2 <- predict(knn2, telco_test_normal[-31])

# Create crosstable, change headers of the table
CrossTable(as.factor(telco_test_normal$Churn), knn_pred2, prop.chisq = F, dnn = c("Real Churn", "Predicted Churn"))

# Create a confusion matrix
knn2_cm <- confusionMatrix(data = knn_pred2, reference = as.factor(telco_test_normal$Churn), positive = "1")

knn2_cm

# Get accuracy and kappa values
knn2_acc <- as.numeric(knn2_cm$overall['Accuracy'])
knn2_kappa <- as.numeric(knn2_cm$overall['Kappa'])
knn2_sens <- as.numeric(knn2_cm$byClass['Sensitivity'])
```
<br>
> The improved kNN achieved a higher accuracy (`r knn2_acc`) and a higher Kappa value (`r knn2_kappa`).

## Decision Tree
### Simple Decision Tree
```{r, cache = TRUE}
# Build a decision tree model
dt <- C5.0(as.factor(Churn) ~ ., data = telco_train_normal)
dtpred <- predict(dt, telco_test_normal[-31])

# Build a crosstable
CrossTable(x = telco_test_normal$Churn, y = dtpred, prop.chisq = F, dnn = c("Actual Churn", "Predicted Churn"))

# Build a confusion matrix
dt_cm <- confusionMatrix(data = dtpred, reference = as.factor(telco_test_normal$Churn), positive = "1")

dt_cm

# Get accuracy and kappa values
dt_acc <- as.numeric(dt_cm$overall['Accuracy'])
dt_kappa <- as.numeric(dt_cm$overall['Kappa'])
dt_sens <- as.numeric(dt_cm$byClass['Sensitivity'])
```
<br>
> A model based on the C5.0 package saw an accuracy of `r dt_acc` and a Kappa value of `r dt_kappa`. We will now try to build decision tree using the Caret package.

### Decision Tree with Caret
```{r, cache = TRUE}
set.seed(300)

## Create a simple tuned model with automated parameter tuning of C5.0 decision tree 
dt2 <- train(Churn ~ ., data = telco_train_normal, method = "C5.0")

# Apply the best candidate model to make predictions
dt_pred2 <- predict(dt2, telco_test_normal[-31])

# Create a confusion matrix
dt2_cm <- confusionMatrix(data = dt_pred2, reference = as.factor(telco_test_normal$Churn), positive = "1")

dt2_cm

# Get accuracy and kappa values
dt2_acc <- as.numeric(dt2_cm$overall['Accuracy'])
dt2_kappa <- as.numeric(dt2_cm$overall['Kappa'])
dt2_sens <- as.numeric(dt2_cm$byClass['Sensitivity'])
```
<br>
  > A model based on the Caret package saw a slightly higher accuracy (`r dt2_acc`)  but a lower Kappa value (`r dt2_kappa`). We will now try to tune the hyperparameters of the model.

```{r, cache = TRUE}
set.seed(300)

## Customize the tuning process using trainControl() to alter resampling strategy
ctrl <- trainControl(method = "cv", number = 10,
                     selectionFunction = "oneSE")

# Use expand.grid() to create grid of tuning parameters
grid <- expand.grid(.model = "tree",
                    .trials = c(1, 5, 10, 15, 20, 25, 30),
                    .winnow = "FALSE")

# Customize train() with the control list and grid of parameters 
dt3 <- train(Churn ~ ., data = telco_train_normal, method = "C5.0",
           metric = "Kappa",
           trControl = ctrl,
           tuneGrid = grid)

# Apply the best candidate model to make predictions
dt_pred3 <- predict(dt3, telco_test_normal[-31])

# Create a confusion matrix
dt3_cm <- confusionMatrix(data = dt_pred3, reference = as.factor(telco_test_normal$Churn), positive = "1")

dt3_cm

# Get accuracy and kappa values
dt3_acc <- as.numeric(dt3_cm$overall['Accuracy'])
dt3_kappa <- as.numeric(dt3_cm$overall['Kappa'])
dt3_sens <- as.numeric(dt3_cm$byClass['Sensitivity'])
```
<br>
> The improved model model based on the Caret package saw an accuracy of `r dt3_acc` and a Kappa value of `r dt3_kappa`.

## Support Vector Machine (SVM)

```{r}
# Create an SVM model
svm_model <- ksvm(Churn ~ ., data = telco_train_normal, kernel = "vanilladot")

# Predict the response variable
svm_pred <- predict(svm_model, telco_test_normal)

# Create a crosstable
CrossTable(telco_test_normal$Churn, svm_pred, prop.chisq = F, dnn = c('Actual Churn', 'Predicted Churn'))

# Create a confusion matrix
svm_cm <- confusionMatrix(data = svm_pred, reference = as.factor(telco_test_normal$Churn), positive = "1")

svm_cm

# Get accuracy and kappa values
svm_acc <- as.numeric(svm_cm$overall['Accuracy'])
svm_kappa <- as.numeric(svm_cm$overall['Kappa'])
svm_sens <- as.numeric(svm_cm$byClass['Sensitivity'])
```
<br>
> The SVM model with a vanilladot kernel achieved an accuracy of `r svm_acc` and a Kappa value of `r svm_kappa`.

# Second-Level Model

```{r}
# Combine everything into one big dataframe for our stacked model.
combineddf <- data.frame(lm_pred, ann_predicted_churn3, knn_pred2, dt_pred3, svm_pred, telco_test_normal$Churn)

# Change colnames to something more intuitive
colnames(combineddf) <- c("LR", "ANN", "kNN", "DT", "SVM", "Actual")

# Show summary statistics for the dataframe
summary(combineddf)
```

## Simple Stacked Model
```{r}
# Let's split everything into a train and test set based on 75-25 split (again)
sm_telco_train <- combineddf[1:(0.75*nrow(combineddf)),]
sm_telco_test <- combineddf[(0.75*nrow(combineddf)+1):nrow(combineddf),]

# Run decision tree again!
dt_sm <- C5.0(sm_telco_train[-6], as.factor(sm_telco_train$Actual))

# Plot for visual purposes...
plot(dt_sm)

# Predict the response variable
dt_pred_comb <- predict(dt_sm, sm_telco_test)

# Create a crosstable
CrossTable(sm_telco_test$Actual, dt_pred_comb, prop.chisq = F, dnn = c("Actual Churn", "Predicted Churn"))

# Create a confusion matrix
dt_comb_cm <- confusionMatrix(data = as.factor(dt_pred_comb), reference = as.factor(sm_telco_test$Actual), positive  = "1")

dt_comb_cm

# Get accuracy and kappa values
comb_acc <- as.numeric(dt_comb_cm$overall['Accuracy'])
comb_kappa <- as.numeric(dt_comb_cm$overall['Kappa'])
comb_sens <- as.numeric(dt_comb_cm$byClass['Sensitivity'])
```
<br>
> The combined model achieved an accuracy of `r comb_acc`, a Kappa value of `r comb_kappa`, and a sensitivity score of `r comb_sens`.

```{r}
# Combine all accuracy, kappa, and sensitivity values
accuracy <- c(comb_acc, lm_acc, ann3_acc, knn2_acc, dt3_acc, svm_acc)
kappa <- c(comb_kappa, lm_kappa, ann3_kappa, knn2_kappa, dt3_kappa, svm_kappa)
sens <- c(comb_sens, lm_sens, ann3_sens, knn2_sens, dt3_sens, svm_sens)

# Create a dataframe with all values
results <- data.frame(accuracy, kappa, sens)
rownames(results) <- c("Combined", "LR", "ANN", "kNN", "DT", "SVM")
colnames(results) <- c("Accuracy", "Kappa", "Sensitivity")

# Show the created dataframe
results
```
<br>
> Thus, the combined model has achieved a relatively high accuracy score, but its Kappa value is lower than that of ANN while its sensitivity is lower than that of ANN and kNN. That being said, we need to keep in mind that the scores for the combined model are based on smaller dataset. In this project, we are specifically interested in minimizing the number of false negatives - unnoticed instances of churn. We will try to reduce that number (and thus increase the combined model's sensitivity score) using an **error cost matrix**.

## Error Cost Matrix
```{r}
# Create a cost matrix
error_cost <- matrix(c(0, 1, 3, 0), nrow = 2)

# Apply the cost matrix to the tree
comb_dt2 <- C5.0(sm_telco_train[-6], as.factor(sm_telco_train$Actual), costs = error_cost)
comb_dtpred2 <- predict(comb_dt2, sm_telco_test)

# Build a confusion matrix
dt_comb2_cm <- confusionMatrix(data = as.factor(comb_dtpred2), reference = as.factor(sm_telco_test$Actual), positive = "1")

dt_comb2_cm

# Get accuracy and kappa values
dt_comb2_acc <- as.numeric(dt_comb2_cm$overall['Accuracy'])
dt_comb2_kappa <- as.numeric(dt_comb2_cm$overall['Kappa'])
dt_comb2_sens <- as.numeric(dt_comb2_cm$byClass['Sensitivity'])
```
<br>
> By applying a cost matrix, we can increase our senstivity score to `r dt_comb2_sens` but the accuracy rate falls to `r dt_comb2_acc`. That is primarily because we are increasing the number of false positives. The goal of our project is to identify individuals with a high risk of churn and target them with retention strategies. In our next submission for this project, we will look into the costs associated with losing customers as well as running such retention strategies. With that information, we will be able to estimate how much money the company will be able to save by targeting only customers who are at risk of churn according to the model.

```{r}
# Extract numbers from the table
TN = as.numeric(dt_comb2_cm$table)[1]
FP = as.numeric(dt_comb2_cm$table)[2]
FN = as.numeric(dt_comb2_cm$table)[3]
TP = as.numeric(dt_comb2_cm$table)[4]

customers_at_risk = TP + FN
customers_targeted = TP + FP
```

